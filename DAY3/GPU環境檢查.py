import torch
import subprocess
import sys

def check_gpu_environment():
    """å®Œæ•´GPUç’°å¢ƒæª¢æŸ¥"""
    
    print("="*60)
    print("GPUç’°å¢ƒæª¢æŸ¥å ±å‘Š")
    print("="*60)
    
    # 1. GPUç¡¬é«”
    print("\nâœ… GPUç¡¬é«”:")
    print("  - NVIDIA GeForce RTX 4060")
    print("  - é¡¯å­˜: 8GB")
    print("  - é©…å‹•: 576.28")
    print("  - CUDA: 12.9")
    
    # 2. PyTorchæª¢æŸ¥
    print("\nğŸ“¦ PyTorchç‹€æ…‹:")
    print(f"  - PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    if torch.cuda.is_available():
        print(f"  âœ… CUDAå¯ç”¨")
        print(f"  - PyTorch CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  - GPUæ•¸é‡: {torch.cuda.device_count()}")
        print(f"  - ç•¶å‰GPU: {torch.cuda.get_device_name(0)}")
        
        # cuDNNæª¢æŸ¥
        if torch.backends.cudnn.is_available():
            print(f"  âœ… cuDNNå¯ç”¨")
            print(f"  - cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        else:
            print(f"  âŒ cuDNNä¸å¯ç”¨")
    else:
        print(f"  âŒ CUDAä¸å¯ç”¨ - å¯èƒ½å®‰è£äº†CPUç‰ˆPyTorch")
        
    # 3. æ¸¬è©¦GPUé‹ç®—
    print("\nğŸ§ª GPUé‹ç®—æ¸¬è©¦:")
    try:
        if torch.cuda.is_available():
            x = torch.randn(100, 100).cuda()
            y = torch.matmul(x, x)
            print(f"  âœ… GPUé‹ç®—æˆåŠŸ")
            print(f"  - æ¸¬è©¦çŸ©é™£é‹ç®—: {x.shape} @ {x.shape} = {y.shape}")
        else:
            print(f"  âŒ ç„¡æ³•ä½¿ç”¨GPUé‹ç®—")
    except Exception as e:
        print(f"  âŒ GPUé‹ç®—å¤±æ•—: {e}")
        
    return torch.cuda.is_available()

# åŸ·è¡Œæª¢æŸ¥
gpu_available = check_gpu_environment()